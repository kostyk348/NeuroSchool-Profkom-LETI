

Модуль 2: Генерация изображений с помощью ИИ: От абстракции к фотореализму


# Модуль 02: Генерация изображений с помощью ИИ: От абстракции к фотореализму

## Цели модуля:

- Познакомиться с принципами работы генеративных моделей изображений (GAN, диффузионные модели).
- Освоить практические навыки создания изображений с помощью ведущих ИИ-инструментов: Midjourney v6, Stable Diffusion 3, DALL-E 3, Leonardo.ai, Ideogram, и других.
- Научиться формулировать эффективные текстовые промпты для генерации изображений различных стилей, жанров и уровней сложности.
- Разобраться с параметрами генерации изображений (разрешение, соотношение сторон, уровень стилизации и т.д.) и научиться управлять ими.
- Освоить техники inpainting и outpainting для редактирования и расширения сгенерированных изображений.
- Узнать о новых возможностях и трендах в области генерации изображений с помощью ИИ.

## Содержание:

1.  **Введение в генерацию изображений с помощью ИИ:**
    *   Краткая история развития генеративных моделей изображений.
    *   Основные типы архитектур: GAN (Generative Adversarial Networks), VAE (Variational Autoencoders), диффузионные модели (Diffusion Models).
    *   Принципы работы GAN: генератор и дискриминатор.
    *   Принципы работы диффузионных моделей: прямое и обратное зашумление.
    *   Преимущества и недостатки разных типов архитектур.

2.  **Обзор инструментов для генерации изображений:**
    *   **Midjourney v6:** Особенности, преимущества, примеры использования, работа через Discord.
    *   **Stable Diffusion 3:**  Особенности, открытый исходный код, возможности локальной установки, ControlNet, LoRA.
    *   **DALL-E 3:**  Интеграция с ChatGPT, высокое качество, особенности использования.
    *   **Leonardo.ai:**  Удобный интерфейс, различные модели, фокус на игровых ассетах.
    *   **Ideogram:** Простота использования, генерация текста на изображениях.
    *   **Другие инструменты:** Playground AI, Recraft, FreePik AI Image Generator, BotHub, Mage.space, Craiyon, NightCafe.

3.  **Текстовые промпты для генерации изображений:**
    *   Структура промпта: объект, действие, окружение, стиль, освещение, композиция.
    *   Использование прилагательных и наречий для детализации.
    *   Указание художественных стилей: импрессионизм, кубизм, фотореализм, аниме, киберпанк и т.д.
    *   Использование имен художников и референсных изображений.
    *   Продвинутые техники: negative prompting, weight (явное указание веса слов в промпте).

4.  **Параметры генерации:**
    *   Разрешение (resolution) и соотношение сторон (aspect ratio).
    *   Уровень стилизации (stylize).
    *   Seed (зерно генерации).
    *   Guidance scale (CFG Scale) – сила влияния промпта.
    *   Steps (количество шагов генерации).
    *   Sampler (метод сэмплирования).

5.  **Inpainting и Outpainting:**
    *   Что такое inpainting и outpainting?
    *   Inpainting: редактирование части изображения, замена объектов.
    *   Outpainting: расширение изображения за пределы исходных границ.
    *   Использование inpainting и outpainting в Midjourney, Stable Diffusion, DALL-E 3.

6.  **Новинки и тренды:**
    *   Улучшение фотореализма и детализации.
    *   Более точное следование сложным промптам.
    *   Генерация изображений с высоким разрешением.
    *   Развитие инструментов для редактирования изображений.
    *   Специализация моделей на определенных стилях.


modules/module_02/theory.md

# Теоретическая часть

## 1. Введение в генерацию изображений с помощью ИИ

**Краткая история:**

Генерация изображений с помощью ИИ – относительно молодая, но бурно развивающаяся область.  Первые значимые результаты были получены в 2014 году с появлением *генеративно-состязательных сетей* (GAN).  С тех пор было предложено множество различных архитектур и подходов, но GAN и *диффузионные модели* остаются наиболее популярными и эффективными.

**Основные типы архитектур:**

*   **GAN (Generative Adversarial Networks):**  Состоят из двух нейронных сетей: *генератора* и *дискриминатора*.  Генератор создает изображения, а дискриминатор пытается отличить их от реальных.  Обе сети обучаются совместно, соревнуясь друг с другом.  В результате генератор учится создавать все более реалистичные изображения.
*   **VAE (Variational Autoencoders):**  Кодируют входное изображение в скрытое (латентное) пространство, а затем декодируют его обратно в изображение.  Обучаются минимизировать разницу между входным и выходным изображением.  VAE, как правило, генерируют менее четкие изображения, чем GAN, но более стабильны в обучении.
*   **Диффузионные модели (Diffusion Models):**  Работают в два этапа: *прямое зашумление* (добавление шума к изображению до тех пор, пока оно не превратится в чистый шум) и *обратное зашумление* (постепенное удаление шума из случайного шума для получения изображения).  Диффузионные модели, такие как Stable Diffusion, DALL-E и Midjourney, в настоящее время являются *лидерами* в области генерации изображений по текстовому описанию.

**Принципы работы GAN:**

*   **Генератор (Generator):**  Принимает на вход случайный шум и преобразует его в изображение.
*   **Дискриминатор (Discriminator):**  Принимает на вход изображение (реальное или сгенерированное генератором) и пытается определить, является ли оно реальным или поддельным.
*   **Состязательное обучение (Adversarial Training):**  Генератор и дискриминатор обучаются одновременно.  Генератор стремится обмануть дискриминатор, создавая все более реалистичные изображения, а дискриминатор стремится стать лучше в распознавании подделок.  Этот процесс продолжается итеративно, пока генератор не начнет создавать изображения, неотличимые от реальных.

**Принципы работы диффузионных моделей:**

*   **Прямое зашумление (Forward Diffusion Process):**  К реальному изображению постепенно добавляется шум, пока оно не превратится в чистый шум.  Этот процесс описывается *марковской цепью*.
*   **Обратное зашумление (Reverse Diffusion Process):**  Нейронная сеть обучается *обращать* процесс зашумления, то есть постепенно удалять шум из случайного шума, чтобы получить изображение.  Этот процесс также описывается марковской цепью.
*   **Обучение:**  Модель обучается предсказывать *добавленный шум* на каждом шаге прямого зашумления.  Это позволяет ей затем "обращать" процесс и генерировать изображения из шума.
* **Текстовое управление**: Для генерации по текстовому описанию, текст кодируется и подаётся на вход сети, управляя процессом "обратного зашумления".

**Преимущества и недостатки:**

| Архитектура      | Преимущества                                                                                                                                                              | Недостатки                                                                                                                              |
| :---------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------- |
| GAN              | Высокая скорость генерации, четкие изображения.                                                                                                                             | Нестабильность обучения (сложно сбалансировать генератор и дискриминатор), "mode collapse" (генератор создает ограниченный набор изображений). |
| VAE              | Стабильность обучения, возможность исследовать скрытое пространство.                                                                                                           | Менее четкие изображения, чем у GAN.                                                                                                    |
| Диффузионные модели | Высокое качество и разнообразие изображений, возможность управления процессом генерации с помощью текстовых промптов, более стабильное обучение по сравнению с GAN.        | Более медленная генерация, чем у GAN, требует больших вычислительных ресурсов.                                                              |

## 2. Обзор инструментов для генерации изображений

... (Подробный обзор Midjourney v6, Stable Diffusion 3, DALL-E 3, Leonardo.ai, Ideogram, Playground AI, Recraft, FreePik AI Image Generator, BotHub, Mage.space, Craiyon, NightCafe, с указанием особенностей, преимуществ, примеров использования, ссылок. Подробный разбор работы в Midjourney через Discord, возможностей локальной установки Stable Diffusion, ControlNet, LoRA.)

## 3. Текстовые промпты для генерации изображений

... (Подробное описание структуры промпта (объект, действие, окружение, стиль, освещение, композиция), использование прилагательных и наречий, указание художественных стилей, использование имен художников и референсных изображений, продвинутые техники (negative prompting, weight). Примеры для разных инструментов и стилей.)

## 4. Параметры генерации

... (Подробное описание параметров генерации (разрешение, соотношение сторон, уровень стилизации, seed, guidance scale, steps, sampler) и их влияния на результат.  Примеры использования для разных инструментов.)

## 5. Inpainting и Outpainting
... (Подробное описание inpainting и outpainting, примеры использования в Midjourney, Stable Diffusion, DALL-E 3, Leonardo.ai.)

## 6. Новинки и тренды
... (Описание последних достижений в области генерации изображений: улучшение фотореализма, более точное следование промптам, генерация в высоком разрешении, развитие inpainting/outpainting, появление новых моделей и инструментов.)


# Практическая часть

## Пошаговое руководство: Создание изображения в Midjourney v6

1.  **Доступ к Midjourney:** Подключитесь к Discord-серверу Midjourney (требуется платная подписка).  Найдите один из каналов для новичков (обычно начинаются с "newbies-").

2.  **Команда /imagine:** В чате введите команду `/imagine` и начните вводить свой промпт.  Например:
    ```
    /imagine a fluffy cat wearing a spacesuit, floating in space, cosmic background, digital art, highly detailed, 4k
    ```

3.  **Структура промпта:**
    *   **Объект:** `a fluffy cat`
    *   **Действие:** `wearing a spacesuit, floating in space`
    *   **Окружение:** `cosmic background`
    *   **Стиль:** `digital art`
    *   **Детали:** `highly detailed`
    *   **Разрешение:** `4k`

4.  **Параметры (добавляются через двойное тире):**
    *   `--ar 3:2` (соотношение сторон 3:2)
    *   `--v 6` (использовать версию Midjourney v6)
    *    `--style raw` (отключить "художественную" обработку Midjourney, получить более буквальный результат)
    *    `--stylize 100` (уровень стилизации, от 0 до 1000; по умолчанию 100)
     Пример с параметрами:
     ```
    /imagine a fluffy cat wearing a spacesuit, floating in space, cosmic background, digital art, highly detailed, 4k --ar 3:2 --v 6 --style raw
     ```

5.  **Использование имен художников:**
    ```
    /imagine a landscape in the style of Van Gogh, starry night, vibrant colors
    ```

6.  **Negative prompting (исключение нежелательных элементов):**
    ```
    /imagine a beautiful forest --no cartoon --no blurry
    ```
     (Исключаем мультяшный стиль и размытость)

7.  **Вес слов (word weighting):**
      ```
      /imagine a cat::2 sitting on a sofa::1
      ```
      (Слово "cat" будет иметь вдвое больший вес, чем "sofa")

8.  **Вариации и апскейлинг:** После генерации вы увидите 4 варианта изображения.
    *   `U1`, `U2`, `U3`, `U4` - кнопки для апскейлинга (увеличения разрешения) выбранного варианта.
    *   `V1`, `V2`, `V3`, `V4` - кнопки для создания вариаций на основе выбранного варианта.
    *   Кнопка с круговыми стрелками (🔄) - повторный запуск генерации с тем же промптом (новый seed).

9.  **Сохранение:** Щелкните правой кнопкой мыши на увеличенном изображении и выберите "Сохранить изображение".

## Пошаговое руководство: Генерация изображений в Stable Diffusion 3 (локальная установка с Automatic1111)

1.  **Установка:** Следуйте инструкциям по установке Automatic1111 (WebUI для Stable Diffusion) для вашей операционной системы.  Потребуется установить Python, Git и скачать репозиторий Automatic1111. ([https://github.com/AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)).  Также потребуется скачать модель Stable Diffusion 3 (например, с Hugging Face [https://huggingface.co/stabilityai](https://huggingface.co/stabilityai)).
2.  **Запуск:** Запустите `webui-user.bat` (Windows) или `webui.sh` (Linux/macOS).  Откройте веб-интерфейс в браузере (обычно по адресу `http://127.0.0.1:7860`).
3.  **Вкладка "txt2img":**
    *   **Prompt:** Введите ваш текстовый промпт.
    *   **Negative prompt:** Введите нежелательные элементы.
    *   **Sampling method:** Выберите метод сэмплирования (например, Euler a, DPM++ 2M Karras).
    *   **Sampling steps:** Укажите количество шагов генерации (обычно 20-50).
    *   **Width/Height:** Укажите ширину и высоту изображения.
    *   **CFG Scale:** Укажите силу влияния промпта (обычно 7-12).
    *   **Seed:**  Зерно генерации (-1 для случайного).
    *   **Batch count/Batch size:** Количество генерируемых изображений.
4. **Пример промпта:**
      ```
      photorealistic portrait of a beautiful woman, intricate details, soft lighting, 8k, artstation
      ```
     **Negative prompt:**
     ```
      ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, watermark, grainy, signature, cut off, draft
     ```

5. **ControlNet:**
    *   ControlNet - это расширение для Stable Diffusion, которое позволяет более точно управлять процессом генерации, используя дополнительные входные данные (например, контурные карты, карты глубины, позы человека).
    *  Установите расширение ControlNet через вкладку "Extensions" в Automatic1111.
    *  Скачайте модели ControlNet ([https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main](https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main)).
    *  Загрузите изображение в ControlNet, выберите препроцессор и модель.
6.  **LoRA (Low-Rank Adaptation):**
    *   LoRA - это небольшие модели, которые "дообучают" основную модель Stable Diffusion для генерации изображений в определенном стиле или с определенными объектами.
    *   Скачайте LoRA-модели (например, с Civitai [https://civitai.com/](https://civitai.com/)).
    *   Поместите LoRA-файлы в папку `models/Lora` в директории Automatic1111.
    *   Используйте синтаксис `<lora:имя_файла:вес>` в промпте.  Например: `<lora:my_lora:0.8>`.
7. **Inpainting (редактирование части изображения).**
    * Перейдите во вкладку img2img.
    * Загрузите изображение.
    * Кистью закрасьте область, которую хотите изменить.
    * Введите промпт, описывающий, что вы хотите получить в этой области.
8. **Outpainting (расширение изображения).**
     * Перейдите во вкладку img2img.
     * Загрузите изображение.
     * Выберите скрипт "Outpainting mk2".
     * Укажите, на сколько пикселей нужно расширить изображение в каждом направлении.
     * Введите промпт, описывающий, что должно быть на добавленной области.
## Пошаговое руководство: Использование DALL-E 3 (через ChatGPT Plus)

1.  **Доступ:** Подпишитесь на ChatGPT Plus.
2.  **Новый чат:** Начните новый чат с ChatGPT.
3.  **Промпт:**  Просто введите текстовый промпт, описывающий желаемое изображение.  Например:
    ```
    Нарисуй картину маслом, изображающую горный пейзаж с озером и заснеженными вершинами на рассвете.
    ```
4.  **Уточнение:** Вы можете уточнять промпт в процессе диалога с ChatGPT.  Например:
    ```
    Сделай озеро более бирюзовым, а горы - более детализированными.
    ```
5. **Сохранение:** Щелкните правой кнопкой мыши на изображении и выберите "Сохранить изображение".

## Кейс: Создание серии иллюстраций для детской книги

... (Разбор кейса: постановка задачи, выбор инструмента (например, Midjourney), разработка персонажей, создание иллюстраций для разных сцен, использование inpainting и outpainting для доработки деталей.)
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Markdown
IGNORE_WHEN_COPYING_END

modules/module_02/homework.md

# Домашнее задание

1.  **"Один сюжет, разные стили":**
    *   Выберите одну сцену (например, "Заброшенный замок на вершине холма под лунным светом").
    *   Используя *как минимум три* разных инструмента (например, Midjourney, Stable Diffusion, DALL-E 3, Leonardo.ai, Ideogram), сгенерируйте изображения этой сцены в *пяти разных стилях*:
        1.  Фотореализм.
        2.  Импрессионизм.
        3.  Аниме.
        4.  Киберпанк.
        5.  Стиль, вдохновленный конкретным художником (например, Сальвадор Дали, Казимир Малевич).
    *   Для каждого изображения сохраните:
        *   Скриншот или ссылку на изображение.
        *   Полный промпт, который вы использовали.
        *   Название инструмента и использованные параметры.
    *   Сравните результаты, полученные с помощью разных инструментов и в разных стилях.  Опишите ваши наблюдения в файле `homework.md`.  Какие инструменты лучше справляются с определенными стилями?

2.  **Inpainting и Outpainting:**
    *   Выберите одно из сгенерированных вами изображений (или возьмите любое другое изображение).
    *   Используя функцию *inpainting*, замените один объект на изображении (например, добавьте дракона на крышу замка).
    *   Используя функцию *outpainting*, расширьте изображение (например, добавьте лес вокруг холма).
    *   Сохраните исходное изображение, изображение после inpainting и изображение после outpainting.  Опишите процесс в файле `homework.md`.

3.  **ControlNet (для Stable Diffusion):**
    *   Если вы используете Stable Diffusion (локально или через облачный сервис), попробуйте использовать ControlNet.
    *   Найдите изображение с четким контуром (например, рисунок здания или человека).
    *   Используйте ControlNet (с препроцессором canny или openpose) для генерации изображения, соответствующего этому контуру.
    *   Опишите процесс и результаты в файле `homework.md`.

4. **LoRA (для Stable Diffusion):**
     * Найдите и скачайте LoRA модель.
     * Сгенерируйте изображение с использованием этой LoRA модели.
     * Сравните с изображением без LoRA.

5.  **Рефлексия:**
    *   Опишите, какие инструменты вам понравились больше всего и почему.
    *   С какими сложностями вы столкнулись при выполнении задания?

**Критерии оценки:**

*   **Полнота:** Выполнены все части задания.
*   **Разнообразие:** Использованы разные инструменты и стили.
*   **Качество:** Сгенерированные изображения соответствуют промптам и имеют хорошее качество.
*   **Техническое исполнение:** Правильно использованы параметры генерации, inpainting, outpainting, ControlNet (если применимо).
*   **Анализ:** Проведен сравнительный анализ результатов, сделаны обоснованные выводы.
*   **Оформление:** Результаты оформлены аккуратно и понятно.
* **Рефлексия**: Присутствует.
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Markdown
IGNORE_WHEN_COPYING_END

modules/module_02/resources.md

# Дополнительные материалы

## Инструменты:

- **Midjourney:** [https://www.midjourney.com/](https://www.midjourney.com/)
- **Stable Diffusion:**
    - Официальный сайт: [https://stability.ai/](https://stability.ai/)
    - Automatic1111 (WebUI): [https://github.com/AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)
    - Hugging Face (модели): [https://huggingface.co/stabilityai](https://huggingface.co/stabilityai)
    - ComfyUI: [https://github.com/comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
    - InvokeAI: [https://www.invoke.ai/](https://www.invoke.ai/)
- **DALL-E 3:** [https://openai.com/dall-e-3](https://openai.com/dall-e-3) (доступен через ChatGPT Plus)
- **Leonardo.ai:** [https://leonardo.ai/](https://leonardo.ai/)
- **Ideogram:** [https://ideogram.ai/](https://ideogram.ai/)
- **Playground AI:** [https://playgroundai.com/](https://playgroundai.com/)
- **Recraft:** [https://www.recraft.ai/](https://www.recraft.ai/)
- **FreePik AI Image Generator:** [https://www.freepik.com/ai/image-generator](https://www.freepik.com/ai/image-generator)
- **BotHub:** [https://bothub.ai/](https://bothub.ai/)
- **Mage.space:** [https://www.mage.space/](https://www.mage.space/)
- **Craiyon:** [https://www.craiyon.com/](https://www.craiyon.com/)
- **NightCafe:** [https://creator.nightcafe.studio/](https://creator.nightcafe.studio/)

## Ресурсы по Stable Diffusion:

- **Civitai:** [https://civitai.com/](https://civitai.com/) (модели, LoRA)
- **Hugging Face:** [https://huggingface.co/](https://huggingface.co/) (модели, датасеты)
- **ControlNet models:** [https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main](https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main)

## Статьи и руководства:

- **Stable Diffusion prompt: a definitive guide:** [https://stable-diffusion-art.com/prompt-guide/](https://stable-diffusion-art.com/prompt-guide/)
- **The Ultimate Guide to Midjourney:** [https://medium.com/mlearning-ai/the-ultimate-guide-to-midjourney-v5-2-56e29477b27f](https://medium.com/mlearning-ai/the-ultimate-guide-to-midjourney-v5-2-56e29477b27f)
- **How to use DALL·E 2 (and the new DALL·E 3):** [https://zapier.com/blog/how-to-use-dall-e/](https://zapier.com/blog/how-to-use-dall-e/)
- **Leonardo.Ai - The ONLY AI Image Generator You'll EVER Need?!:** [https://www.youtube.com/watch?v=YMg756Ds_24](https://www.youtube.com/watch?v=YMg756Ds_24)(видео)

## Глоссарий:

*   **GAN (Generative Adversarial Network):** Генеративно-состязательная сеть.
*   **VAE (Variational Autoencoder):** Вариационный автоэнкодер.
*   **Диффузионная модель (Diffusion Model):** Модель, основанная на процессе зашумления и обратного зашумления.
*   **Inpainting:** Редактирование части изображения.
*   **Outpainting:** Расширение изображения за пределы исходных границ.
*   **ControlNet:** Расширение для Stable Diffusion, позволяющее управлять генерацией с помощью дополнительных входных данных.
*   **LoRA (Low-Rank Adaptation):** Небольшая модель, "дообучающая" основную модель Stable Diffusion.
*   **Seed:** Зерно генерации (число, определяющее начальное состояние генератора случайных чисел).
*   **Guidance scale (CFG Scale):** Сила влияния промпта на генерацию.
*   **Steps:** Количество шагов генерации.
*   **Sampler:** Метод сэмплирования (алгоритм, используемый для генерации изображения из шума).
*   **Negative prompt:** Текстовый промпт, описывающий то, чего *не должно быть* на изображении.
* **Prompt weighting**: Назначение весов словам в промпте.
